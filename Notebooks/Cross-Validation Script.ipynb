{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform 5-fold CV\n",
    "\n",
    "- In a script: \n",
    "    - First do the train-test split\n",
    "    - Then, Stratified KFold\n",
    "    - For each split,\n",
    "        - Save the train set\n",
    "        - Save the val set\n",
    "\n",
    "- For each split, run a model for that \n",
    "    - Record results somewhere\n",
    "    - Don't need to save model\n",
    "\n",
    "- Once you have done that\n",
    "    - Do a final training with the full training dataset\n",
    "    - Evaluate with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "data_filepath = r\"C:\\Users\\ammar\\Documents\\Python ML neuropysch disease\\Diagnosing-ADHD-With-ConvLSTM-master\\References\\model_data.csv\"\n",
    "training_filepath = r\"C:\\Users\\ammar\\Documents\\Python ML neuropysch disease\\Diagnosing-ADHD-With-ConvLSTM-master\\Data\\training_data_{}\"\n",
    "validation_filepath = r\"C:\\Users\\ammar\\Documents\\Python ML neuropysch disease\\Diagnosing-ADHD-With-ConvLSTM-master\\Data\\validatation_data_{}\"\n",
    "testing_filepath = r\"C:\\Users\\ammar\\Documents\\Python ML neuropysch disease\\Diagnosing-ADHD-With-ConvLSTM-master\\Datatesting_data\"\n",
    "\n",
    "model_csv = data_filepath\n",
    "model_data = pd.read_csv(model_csv)\n",
    "\n",
    "X = model_data['Image']\n",
    "y = model_data['DX']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "\n",
    "count = 1\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_index, val_index in skf.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    model_train_data = pd.DataFrame(pd.concat([X_train_fold, y_train_fold], axis=1))\n",
    "    model_val_data = pd.DataFrame(pd.concat([X_val_fold, y_val_fold], axis=1))\n",
    "    \n",
    "    model_train_data.to_csv(training_filepath.format(count), index=False)\n",
    "    model_val_data.to_csv(validation_filepath.format(count), index=False)\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "model_test_data = pd.DataFrame(pd.concat([X_test, y_test], axis=1))\n",
    "model_test_data.to_csv(testing_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from Code.data_generator import FMRIDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FMRIDataGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m validate_steps_per_epoch \u001b[38;5;241m=\u001b[39m model_val_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Generators\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m training_generator \u001b[38;5;241m=\u001b[39m \u001b[43mFMRIDataGenerator\u001b[49m(partition[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], train_labels, dataset_dir, batch_size)\n\u001b[0;32m     53\u001b[0m validation_generator \u001b[38;5;241m=\u001b[39m FMRIDataGenerator(partition[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m], val_labels, dataset_dir, batch_size)\n\u001b[0;32m     55\u001b[0m curr_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m%H-%M-%S%z_%m%d%Y\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FMRIDataGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Conv3D, MaxPool3D, TimeDistributed, Flatten, LSTM, Dense\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "\n",
    "# ============================ DATA WORK ============================\n",
    "\n",
    "# Dataframes\n",
    "file_num = \"peking\" #1\n",
    "dataset_dir = r\"D:/Peking_1\"\n",
    "model_train_data = pd.read_csv(r\"C:\\Users\\ammar\\Documents\\Python ML neuropysch disease\\Diagnosing-ADHD-With-ConvLSTM-master\\Data\\training_{}.csv\".format(file_num) )\n",
    "model_val_data = pd.read_csv(r\"C:\\Users\\ammar\\Documents\\Python ML neuropysch disease\\Diagnosing-ADHD-With-ConvLSTM-master\\Data\\validation_{}.csv\".format(file_num) )\n",
    "\n",
    "# Dictionary of data values\n",
    "partition = {'train': model_train_data['Image'].values, \n",
    "             'validation': model_val_data['Image'].values}\n",
    "\n",
    "# Training Data\n",
    "train_labels = {}\n",
    "for index, row in model_train_data.iterrows():\n",
    "    train_labels[row['Image']] = row['DX']\n",
    "    \n",
    "# Validation Data\n",
    "val_labels = {}\n",
    "for index, row in model_val_data.iterrows():\n",
    "    val_labels[row['Image']] = row['DX']\n",
    "\n",
    "# ============================ MODEL META ============================\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 6\n",
    "input_shape=(177,28,28,28,1)\n",
    "\n",
    "train_steps_per_epoch = model_train_data.shape[0] // batch_size\n",
    "validate_steps_per_epoch = model_val_data.shape[0] // batch_size\n",
    "\n",
    "# Generators\n",
    "training_generator = FMRIDataGenerator(partition['train'], train_labels, dataset_dir, batch_size)\n",
    "validation_generator = FMRIDataGenerator(partition['validation'], val_labels, dataset_dir, batch_size)\n",
    "\n",
    "curr_time = f'{datetime.now():%H-%M-%S%z_%m%d%Y}'\n",
    "logger_path = \"/pylon5/cc5614p/deopha32/Saved_Models/adhd-fmri-history_cv{num}_{time}.csv\".format(num=file_num,time=curr_time)\n",
    "\n",
    "csv_logger = CSVLogger(logger_path, append=True)\n",
    "\n",
    "callbacks = [csv_logger]\n",
    "\n",
    "# ============================ MODEL ARCHITECTURE ============================\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    cnn_lstm_model = Sequential()\n",
    "\n",
    "    cnn_lstm_model.add(TimeDistributed(Conv3D(filters=64,kernel_size=(3,3,3),activation='relu'),\n",
    "                                  input_shape=input_shape, name=\"Input_Conv_Layer\"))\n",
    "\n",
    "    cnn_lstm_model.add(TimeDistributed(MaxPool3D(\n",
    "                                    pool_size=(2, 2, 2),\n",
    "                                    strides=(2, 2, 2),\n",
    "                                    padding='valid'\n",
    "                                    ), name=\"Pool_Layer_1\"))\n",
    "\n",
    "    cnn_lstm_model.add(TimeDistributed(Flatten(), name=\"Flatten_Layer\"))\n",
    "    \n",
    "with tf.device('/cpu:0'):\n",
    "\n",
    "    cnn_lstm_model.add(LSTM(10, dropout = 0.3, recurrent_dropout = 0.3, name=\"LSTM_Layer\"))\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    cnn_lstm_model.add(Dense(1, activation = 'sigmoid', name=\"Output_Dense_Layer\"))\n",
    "\n",
    "    cnn_lstm_model.compile(optimizer=optimizers.Adam(lr=0.0001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_model.fit_generator(generator=training_generator,\n",
    "    steps_per_epoch=train_steps_per_epoch, verbose=1, callbacks=callbacks,\n",
    "    validation_data=validation_generator, validation_steps=validate_steps_per_epoch,\n",
    "    epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
